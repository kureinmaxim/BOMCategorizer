# -*- coding: utf-8 -*-
"""
–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ BOM —Ñ–∞–π–ª–æ–≤

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –≤—Ö–æ–¥–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:
- .txt (—Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏)
- .docx (–¥–æ–∫—É–º–µ–Ω—Ç—ã Word —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏)
- .xlsx, .xls (Excel —Ñ–∞–π–ª—ã)
"""

import os
import re
import sys
import json
import argparse
from typing import List, Dict, Any, Optional
import pandas as pd

from .parsers import parse_txt_like, parse_docx
from .classifiers import classify_row
from .excel_writer import write_categorized_excel, enrich_with_mr_and_total
from .txt_writer import write_txt_reports
from .utils import normalize_column_names, find_column


def add_excel_row_numbers(df: pd.DataFrame, header_offset: int = 2) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É —Å –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–æ–∫ Excel, –µ—Å–ª–∏ –æ–Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç,
    –∏–ª–∏ –∑–∞–ø–æ–ª–Ω—è–µ—Ç –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–æ–∫
    
    Args:
        df: DataFrame –ø–æ—Å–ª–µ —á—Ç–µ–Ω–∏—è Excel
        header_offset: –°–º–µ—â–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–æ–±—ã—á–Ω–æ 2: —Å—Ç—Ä–æ–∫–∞ 1 = –∑–∞–≥–æ–ª–æ–≤–æ–∫, –¥–∞–Ω–Ω—ã–µ —Å 2)
    
    Returns:
        DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π/–∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π "‚Ññ –ø\\–ø"
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∫–æ–ª–æ–Ω–∫–∞ —Å –Ω–æ–º–µ—Ä–∞–º–∏ –ø–æ–∑–∏—Ü–∏–π
    pp_columns = [col for col in df.columns if str(col).startswith('‚Ññ –ø')]
    
    if not pp_columns:
        # –ö–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç - —Å–æ–∑–¥–∞—ë–º —Å –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–æ–∫ Excel
        df['‚Ññ –ø\\–ø'] = range(header_offset, header_offset + len(df))
        print(f"  [+] –î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ '‚Ññ –ø\\–ø' —Å –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–æ–∫ Excel ({header_offset}-{header_offset + len(df) - 1})")
    else:
        # –ö–æ–ª–æ–Ω–∫–∞ –µ—Å—Ç—å - –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –∏—Ö
        pp_col = pp_columns[0]
        empty_count = df[pp_col].isna().sum()
        
        if empty_count > 0:
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–æ–∫ Excel
            for idx in df[df[pp_col].isna()].index:
                df.loc[idx, pp_col] = header_offset + idx
            print(f"  [+] –ó–∞–ø–æ–ª–Ω–µ–Ω–æ {empty_count} –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–æ–ª–æ–Ω–∫–µ '{pp_col}' –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–æ–∫ Excel")
    
    return df


def load_and_combine_inputs(input_paths: List[str], sheets_str: Optional[str] = None, sheet: Optional[str] = None) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    
    Args:
        input_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –≤—Ö–æ–¥–Ω—ã–º —Ñ–∞–π–ª–∞–º
        sheets_str: –°—Ç—Ä–æ–∫–∞ —Å –Ω–æ–º–µ—Ä–∞–º–∏ –ª–∏—Å—Ç–æ–≤ Excel (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)
        sheet: –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ª–∏—Å—Ç –¥–ª—è —á—Ç–µ–Ω–∏—è
        
    Returns:
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π DataFrame —Å–æ –≤—Å–µ–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    all_rows: List[pd.DataFrame] = []
    
    for input_path in input_paths:
        ext = os.path.splitext(input_path)[1].lower()
        
        # TXT parsing
        if ext in [".txt"]:
            try:
                df_txt = parse_txt_like(input_path)
                df_txt["source_file"] = os.path.basename(input_path)
                df_txt["source_sheet"] = ""
                all_rows.append(df_txt)
            except Exception as exc:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å TXT '{input_path}': {exc}", file=sys.stderr)
        
        # DOCX parsing
        elif ext in [".doc", ".docx"]:
            try:
                df_docx = parse_docx(input_path)
                df_docx["source_file"] = os.path.basename(input_path)
                df_docx["source_sheet"] = ""
                all_rows.append(df_docx)
            except Exception as exc:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å DOCX '{input_path}': {exc}", file=sys.stderr)
        
        # Excel parsing
        elif ext in [".xlsx", ".xls"]:
            try:
                # –ß–∏—Ç–∞—Ç—å "–ö–æ–¥ –ú–†" –∫–∞–∫ —Å—Ç—Ä–æ–∫—É, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —á–∏—Å–µ–ª
                read_kwargs = {
                    'dtype': {
                        '–ö–æ–¥ –ú–†': str,
                        '–∫–æ–¥ –º—Ä': str,
                        '–ö–û–î –ú–†': str,
                        '–ö–æ–¥ –º—Ä': str,
                        '–∫–æ–¥_–º—Ä': str,
                        'kodmr': str
                    }
                }
                
                # Parse sheets parameter if provided
                if sheets_str:
                    sheets_requested = []
                    for s_token in sheets_str.split(","):
                        s_token = s_token.strip()
                        try:
                            sheets_requested.append(int(s_token))
                        except ValueError:
                            sheets_requested.append(s_token)
                    
                    # Read multiple sheets
                    for sh in sheets_requested:
                        read_kwargs_copy = read_kwargs.copy()
                        read_kwargs_copy["sheet_name"] = sh
                        try:
                            dfi = pd.read_excel(input_path, **read_kwargs_copy)
                            
                            if isinstance(dfi, dict):
                                first_key = next(iter(dfi))
                                dfi = dfi[first_key]
                                sh = first_key
                            
                            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—É—é –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
                            unnamed_count = sum(1 for col in dfi.columns if str(col).lower().startswith('unnamed'))
                            has_mostly_unnamed = unnamed_count >= len(dfi.columns) * 0.5
                            
                            header_was_removed = False
                            if has_mostly_unnamed and not dfi.empty and dfi.iloc[0].notna().any():
                                first_row_text = ' '.join(str(val).lower() for val in dfi.iloc[0] if pd.notna(val))
                                looks_like_header = any(keyword in first_row_text for keyword in 
                                    ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–∫–æ–ª.', '–∫–æ–¥', 'description', 'qty'])
                                
                                if looks_like_header:
                                    new_headers = dfi.iloc[0].fillna('').astype(str)
                                    dfi = dfi[1:].reset_index(drop=True)
                                    dfi.columns = new_headers
                                    header_was_removed = True
                            
                            # –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–æ–∫ Excel, –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ "‚Ññ –ø\–ø" –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                            header_offset = 3 if header_was_removed else 2
                            dfi = add_excel_row_numbers(dfi, header_offset)
                            
                            dfi["source_file"] = os.path.basename(input_path)
                            dfi["source_sheet"] = str(sh)
                            all_rows.append(dfi)
                        except Exception as exc:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ª–∏—Å—Ç '{sh}' –∏–∑ '{input_path}': {exc}", file=sys.stderr)
                
                elif sheet is not None:
                    # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∫–∞–∑–∞–ª –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ª–∏—Å—Ç —á–µ—Ä–µ–∑ --sheet
                    try:
                        sheet = int(sheet)
                    except ValueError:
                        pass
                    read_kwargs["sheet_name"] = sheet
                    
                    df = pd.read_excel(input_path, **read_kwargs)
                    if isinstance(df, dict):
                        first_key = next(iter(df))
                        df = df[first_key]
                        src_sheet = first_key
                    else:
                        src_sheet = sheet
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—É—é –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
                    header_was_removed = False
                    if all(str(col).lower().startswith('unnamed') for col in df.columns):
                        if not df.empty and df.iloc[0].notna().any():
                            new_headers = df.iloc[0].fillna('').astype(str)
                            df = df[1:].reset_index(drop=True)
                            df.columns = new_headers
                            header_was_removed = True
                    
                    # –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–æ–∫ Excel, –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ "‚Ññ –ø\–ø" –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                    header_offset = 3 if header_was_removed else 2
                    df = add_excel_row_numbers(df, header_offset)
                    
                    df["source_file"] = os.path.basename(input_path)
                    df["source_sheet"] = str(src_sheet)
                    all_rows.append(df)
                
                else:
                    # –õ–∏—Å—Ç—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã - —á–∏—Ç–∞–µ–º –í–°–ï –ª–∏—Å—Ç—ã
                    all_sheets_data = pd.read_excel(input_path, sheet_name=None, **{k: v for k, v in read_kwargs.items() if k != 'sheet_name'})
                    for sheet_name, df_local in all_sheets_data.items():
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—É—é –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
                        unnamed_count = sum(1 for col in df_local.columns if str(col).lower().startswith('unnamed'))
                        has_mostly_unnamed = unnamed_count >= len(df_local.columns) * 0.5
                        
                        header_was_removed = False
                        if has_mostly_unnamed and not df_local.empty and df_local.iloc[0].notna().any():
                            first_row_text = ' '.join(str(val).lower() for val in df_local.iloc[0] if pd.notna(val))
                            looks_like_header = any(keyword in first_row_text for keyword in 
                                ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–∫–æ–ª.', '–∫–æ–¥', 'description', 'qty'])
                            
                            if looks_like_header:
                                new_headers = df_local.iloc[0].fillna('').astype(str)
                                df_local = df_local[1:].reset_index(drop=True)
                                df_local.columns = new_headers
                                header_was_removed = True
                        
                        # –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–æ–∫ Excel, –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ "‚Ññ –ø\–ø" –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                        header_offset = 3 if header_was_removed else 2
                        df_local = add_excel_row_numbers(df_local, header_offset)
                        
                        df_local["source_file"] = os.path.basename(input_path)
                        df_local["source_sheet"] = str(sheet_name)
                        all_rows.append(df_local)
            
            except Exception as exc:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å Excel '{input_path}': {exc}", file=sys.stderr)
                raise SystemExit(f"Failed to read Excel '{input_path}': {exc}")
    
    if not all_rows:
        raise SystemExit("No data loaded from inputs")
    
    df = pd.concat(all_rows, ignore_index=True)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º source_file –∏ source_sheet –¥–ª—è –º–Ω–æ–≥–æ–ª–∏—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
    if 'source_sheet' in df.columns and 'source_file' in df.columns:
        file_sheet_counts = df.groupby('source_file')['source_sheet'].nunique()
        multi_sheet_files = file_sheet_counts[file_sheet_counts > 1].index.tolist()
        
        if multi_sheet_files:
            for file in multi_sheet_files:
                file_mask = df['source_file'] == file
                unique_sheets = df.loc[file_mask, 'source_sheet'].unique()
                sheet_to_num = {sheet: i+1 for i, sheet in enumerate(unique_sheets)}
                
                df.loc[file_mask, 'source_file'] = df.loc[file_mask].apply(
                    lambda row: f"{row['source_file']} –õ–∏—Å—Ç_{sheet_to_num[row['source_sheet']]}", 
                    axis=1
                )
            
            df = df.drop(columns=['source_sheet'])
    
    return df


def normalize_and_merge_columns(df: pd.DataFrame) -> tuple:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∫–æ–ª–æ–Ω–∫–∏
    
    Returns:
        (df, ref_col, desc_col, value_col, part_col, qty_col, mr_col)
    """
    # Normalize columns
    original_cols = list(df.columns)
    lower_cols = normalize_column_names(original_cols)
    rename_map = {orig: norm for orig, norm in zip(original_cols, lower_cols)}
    df = df.rename(columns=rename_map)
    
    # Common column guesses
    ref_col = find_column(["ref", "reference", "designator", "refdes", "reference designator", "–æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ", "–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ"], list(df.columns))
    desc_col = find_column(["description", "desc", "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏–≤–ø", "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–∏–º—è", "item", "part", "part name", "–Ω–∞–∏–º."], list(df.columns))
    value_col = find_column(["value", "–∑–Ω–∞—á–µ–Ω–∏–µ", "–Ω–æ–º–∏–Ω–∞–ª"], list(df.columns))
    part_col = find_column(["partnumber", "mfr part", "mpn", "pn", "art", "–∞—Ä—Ç–∏–∫—É–ª", "part", "part name"], list(df.columns))
    qty_col = find_column([
        "qty", "quantity", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ", "–∫–æ–ª.", "–∫–æ–ª-–≤–æ", "–∫–æ–ª. –≤ –∫—Ç–¥", "–∫–æ–ª –≤ –∫—Ç–¥", "–∫–æ–ª. –≤ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏", "–∫–æ–ª. –≤ –∫–¥—Ç",
        "–∫–æ–ª. –≤ –∫—Ç–¥", "–∫–æ–ª. –≤ –∫—Ç–¥, —à—Ç", "–∫–æ–ª. –≤ –∫—Ç–¥ (—à—Ç)", "–∫–æ–ª. –≤ –∫—Ç–¥, —à—Ç."
    ], list(df.columns))
    mr_col = find_column([
        "–∫–æ–¥ –º—Ä", "–∫–æ–¥ –∏–≤–ø", "–∫–æ–¥ –º—Ä/–∏–≤–ø", "–∫–æ–¥ –ø–æ–∑–∏—Ü–∏–∏", "–∫–æ–¥ –∏–∑–¥–µ–ª–∏—è", "–∫–æ–¥ –º—Ä –ø–æ–∑–∏—Ü–∏–∏", "–∫–æ–¥ –º—Ä –∏–≤–ø"
    ], list(df.columns))
    
    # Merge multiple description columns
    possible_desc_cols = [col for col in df.columns if any(
        col.startswith(prefix) for prefix in ["description", "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "desc", "–∏–º—è"]
    )]
    
    if len(possible_desc_cols) > 1:
        def merge_desc(row):
            for col in possible_desc_cols:
                val = row.get(col)
                if pd.notna(val) and str(val).strip():
                    return val
            return None
        
        df["_merged_description_"] = df.apply(merge_desc, axis=1)
        for col in possible_desc_cols:
            if col in df.columns:
                df = df.drop(columns=[col])
        desc_col = "_merged_description_"
    
    # Merge multiple qty columns
    possible_qty_cols = [col for col in df.columns if any(
        col.startswith(prefix) for prefix in ["qty", "quantity", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ", "–∫–æ–ª"]
    )]
    
    if len(possible_qty_cols) > 1:
        def merge_qty(row):
            for col in possible_qty_cols:
                val = row.get(col)
                if pd.notna(val):
                    try:
                        return float(val) if val != 0 or str(val).strip() == '0' else None
                    except:
                        pass
            return None
        
        df["_merged_qty_"] = df.apply(merge_qty, axis=1)
        for col in possible_qty_cols:
            if col in df.columns:
                df = df.drop(columns=[col])
        qty_col = "_merged_qty_"
    
    # Ensure we have at least some text to classify
    if not any([ref_col, desc_col, value_col, part_col]):
        df["_row_text_"] = df.apply(lambda r: " ".join(str(x) for x in r.values if pd.notna(x)), axis=1)
        desc_col = "_row_text_"
    
    return df, ref_col, desc_col, value_col, part_col, qty_col, mr_col


def run_classification(df: pd.DataFrame, ref_col: str, desc_col: str, value_col: str, part_col: str, loose: bool) -> pd.DataFrame:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ DataFrame
    
    Returns:
        DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π 'category'
    """
    categories: List[str] = []
    for _, row in df.iterrows():
        ref = row.get(ref_col) if ref_col else None
        desc = row.get(desc_col) if desc_col else None
        val = row.get(value_col) if value_col else None
        part = row.get(part_col) if part_col else None
        src_file = row.get('source_file') if 'source_file' in df.columns else None
        note_val = row.get('note') if 'note' in df.columns else None
        categories.append(classify_row(ref, desc, val, part, strict=not loose, source_file=src_file, note=note_val))
    
    df = df.copy()
    df["category"] = categories
    return df


def apply_rules_from_json(df: pd.DataFrame, rules_json: str, desc_col: str, value_col: str, part_col: str, ref_col: str) -> pd.DataFrame:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑ JSON —Ñ–∞–π–ª–∞
    
    Returns:
        DataFrame —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
    """
    if not os.path.exists(rules_json):
        return df
    
    try:
        with open(rules_json, "r", encoding="utf-8") as f:
            rules = json.load(f)
        
        if not isinstance(rules, list) or len(rules) == 0:
            return df
        
        print(f"–ü—Ä–∏–º–µ–Ω—è—é {len(rules)} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª –∏–∑ {rules_json}...")
        rules_applied_count = 0
        
        for i, rule in enumerate(rules, start=1):
            cat = str(rule.get("category", "")).strip()
            contains = str(rule.get("contains", "")).strip().lower()
            regex = rule.get("regex")
            
            if not cat or (not contains and not regex):
                continue
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∫–æ –í–°–ï–ú —ç–ª–µ–º–µ–Ω—Ç–∞–º —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π unclassified
            mask = df["category"] == "unclassified"
            
            if contains:
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ normalize_and_merge_columns
                def get_col_values(col_name):
                    if col_name and col_name in df.columns:
                        return df[col_name].astype(str).str.lower().fillna("")
                    return pd.Series([""] * len(df))
                
                blob = (
                    get_col_values(desc_col) + " " +
                    get_col_values(value_col) + " " +
                    get_col_values(part_col) + " " +
                    get_col_values(ref_col)
                )
                mask = mask & blob.str.contains(re.escape(contains), na=False)
            
            if regex:
                try:
                    r = re.compile(regex, re.IGNORECASE)
                    
                    def get_col_values_str(col_name):
                        if col_name and col_name in df.columns:
                            return df[col_name].astype(str).fillna("")
                        return pd.Series([""] * len(df))
                    
                    text_series = (
                        get_col_values_str(desc_col) + " " +
                        get_col_values_str(value_col) + " " +
                        get_col_values_str(part_col) + " " +
                        get_col_values_str(ref_col)
                    )
                    mask = mask & text_series.apply(lambda t: bool(r.search(t)))
                except Exception:
                    pass
            
            matched_count = mask.sum()
            if matched_count > 0:
                df.loc[mask, "category"] = cat
                rules_applied_count += matched_count
        
        if rules_applied_count > 0:
            print(f"[OK] {rules_applied_count} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º –ø—Ä–∞–≤–∏–ª–∞–º")
    
    except Exception as exc:
        print(f"[!] –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ {rules_json}: {exc}")
    
    return df


def interactive_classification(df: pd.DataFrame, desc_col: str, value_col: str, part_col: str, rules_json: str, auto_prompted: bool = False) -> pd.DataFrame:
    """
    –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    
    Returns:
        DataFrame —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
    """
    cat_names = [
        ("resistors", "–†–µ–∑–∏—Å—Ç–æ—Ä—ã"),
        ("capacitors", "–ö–æ–Ω–¥–µ–Ω—Å–∞—Ç–æ—Ä—ã"),
        ("inductors", "–î—Ä–æ—Å—Å–µ–ª–∏"),
        ("ics", "–ú–∏–∫—Ä–æ—Å—Ö–µ–º—ã"),
        ("connectors", "–†–∞–∑—ä–µ–º—ã"),
        ("dev_boards", "–û—Ç–ª–∞–¥–æ—á–Ω—ã–µ –ø–ª–∞—Ç—ã"),
        ("semiconductors", "–ü–æ–ª—É–ø—Ä–æ–≤–æ–¥–Ω–∏–∫–∏"),
        ("our_developments", "–ù–∞—à–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏"),
        ("others", "–î—Ä—É–≥–∏–µ"),
        ("unclassified", "–ù–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"),
    ]
    
    uncls = df[df["category"] == "unclassified"].copy()
    max_preview = min(len(uncls), 50)
    
    skip_interactive = False
    if auto_prompted:
        print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(uncls)} –Ω–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤!")
        print(f"–î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è.")
        response = input(f"\n–ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏? (y/n, Enter=y): ").strip().lower()
        if response and response not in ['y', 'yes', '–¥', '–¥–∞', '']:
            print("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –ø—Ä–æ–ø—É—â–µ–Ω. –ù–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –æ—Å—Ç–∞–Ω—É—Ç—Å—è –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '–ù–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'.")
            skip_interactive = True
        else:
            print(f"\n–ù–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: {len(uncls)}. –ü–æ–∫–∞–∂—É –ø–µ—Ä–≤—ã–µ {max_preview} –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏.")
    else:
        print(f"–ù–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: {len(uncls)}. –ü–æ–∫–∞–∂—É –ø–µ—Ä–≤—ã–µ {max_preview} –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏.")
    
    if skip_interactive:
        return df
    
    # Load existing rules
    existing_rules: List[Dict[str, Any]] = []
    if os.path.exists(rules_json):
        try:
            with open(rules_json, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    existing_rules = data
        except Exception:
            pass
    
    for idx, (_, row) in enumerate(uncls.head(max_preview).iterrows(), start=1):
        text_blob = " ".join(str(x) for x in [row.get(desc_col), row.get(value_col), row.get(part_col)] if pd.notna(x))
        print(f"[{idx}] {text_blob}")
        for i, (_, ru) in enumerate(cat_names, start=1):
            print(f"  {i}. {ru}")
        choice = input("–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–º–µ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (Enter —á—Ç–æ–±—ã –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å): ").strip()
        if choice.isdigit():
            ci = int(choice)
            if 1 <= ci <= len(cat_names):
                selected_key = cat_names[ci - 1][0]
                df.loc[uncls.index[idx - 1], "category"] = selected_key
                rule = {"contains": text_blob[:160], "category": selected_key}
                existing_rules.append(rule)
    
    # Save updated rules
    try:
        with open(rules_json, "w", encoding="utf-8") as f:
            json.dump(existing_rules, f, ensure_ascii=False, indent=2)
        print(f"–°–æ—Ö—Ä–∞–Ω–∏–ª –ø—Ä–∞–≤–∏–ª–∞: {rules_json}")
    except Exception as exc:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞: {exc}")
    
    return df


def combine_debug_modules(df: pd.DataFrame) -> pd.DataFrame:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è "–û—Ç–ª–∞–¥–æ—á–Ω—ã–µ –ø–ª–∞—Ç—ã –∏ –º–æ–¥—É–ª–∏"
    
    Returns:
        DataFrame —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
    """
    debug_modules_parts = []
    
    # 1. –ù–∞—à–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    our_dev = df[df["category"] == "our_developments"]
    if not our_dev.empty:
        debug_modules_parts.append(our_dev)
    
    # 2. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
    if debug_modules_parts:
        empty_row = pd.DataFrame([{col: '' for col in df.columns}])
        debug_modules_parts.append(empty_row)
    
    # 3. –û—Ç–ª–∞–¥–æ—á–Ω—ã–µ –ø–ª–∞—Ç—ã
    dev_boards = df[df["category"] == "dev_boards"]
    if not dev_boards.empty:
        debug_modules_parts.append(dev_boards)
    
    # 4. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
    if len(debug_modules_parts) > 0 and not dev_boards.empty:
        empty_row2 = pd.DataFrame([{col: '' for col in df.columns}])
        debug_modules_parts.append(empty_row2)
    
    # 5. –°–í–ß –º–æ–¥—É–ª–∏
    rf_mods = df[df["category"] == "rf_modules"]
    if not rf_mods.empty:
        debug_modules_parts.append(rf_mods)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞—Å—Ç–∏
    debug_modules_combined = pd.concat(debug_modules_parts, ignore_index=True) if debug_modules_parts else pd.DataFrame()
    
    return debug_modules_combined


def create_outputs_dict(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    """
    –°–æ–∑–¥–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –≤—ã—Ö–æ–¥–Ω—ã—Ö DataFrame –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å {category_key: DataFrame}
    """
    debug_modules_combined = combine_debug_modules(df)
    
    outputs = {
        "debug_modules": debug_modules_combined,
        "ics": df[df["category"] == "ics"],
        "resistors": df[df["category"] == "resistors"],
        "capacitors": df[df["category"] == "capacitors"],
        "inductors": df[df["category"] == "inductors"],
        "semiconductors": df[df["category"] == "semiconductors"],
        "connectors": df[df["category"] == "connectors"],
        "optics": df[df["category"] == "optics"],
        "power_modules": df[df["category"] == "power_modules"],
        "cables": df[df["category"] == "cables"],
        "others": df[df["category"] == "others"],
        "unclassified": df[df["category"] == "unclassified"],
    }
    
    return outputs


def print_summary(outputs: Dict[str, pd.DataFrame]):
    """
    –í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–∫—É –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    """
    print("Split complete:")
    for key, part_df in outputs.items():
        print(f"  {key}: {len(part_df)}")


def parse_exclude_items(exclude_file_path: str) -> list:
    """
    –ü–∞—Ä—Å–∏—Ç —Ñ–∞–π–ª —Å —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    
    –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç "–ù–∞–∑–≤–∞–Ω–∏–µ –ò–í–ü, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ"
    –ù–∞–ø—Ä–∏–º–µ—Ä:
        AD9221AR, 2
        GRM1885C1H681J, 1
        
    Args:
        exclude_file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∏—Å–∫–ª—é—á–µ–Ω–∏—è–º–∏
        
    Returns:
        –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–Ω–∞–∑–≤–∞–Ω–∏–µ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)
    """
    exclude_items = []
    
    if not os.path.exists(exclude_file_path):
        print(f"‚ö†Ô∏è –§–∞–π–ª –∏—Å–∫–ª—é—á–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {exclude_file_path}")
        return exclude_items
    
    try:
        with open(exclude_file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                # –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–æ—Ä–º–∞—Ç–∞ "–ù–∞–∑–≤–∞–Ω–∏–µ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ"
                if ',' in line:
                    parts = line.rsplit(',', 1)
                    if len(parts) == 2:
                        name = parts[0].strip()
                        try:
                            qty = int(parts[1].strip())
                            exclude_items.append((name, qty))
                        except ValueError:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {line_num}: –Ω–µ–≤–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ '{parts[1].strip()}'")
                    else:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {line_num}: –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç")
                else:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {line_num}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–ø—è—Ç–∞—è")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π: {e}")
    
    return exclude_items


def apply_exclusions(df: pd.DataFrame, exclude_items: list, desc_col: str) -> pd.DataFrame:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫ DataFrame
    
    Args:
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ BOM
        exclude_items: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–Ω–∞–∑–≤–∞–Ω–∏–µ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ) –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        desc_col: –ò–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º
        
    Returns:
        DataFrame —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–º–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è–º–∏
    """
    if not exclude_items:
        return df
    
    if desc_col not in df.columns:
        print(f"‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ '{desc_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–∫–ª—é—á–µ–Ω–∏—è –Ω–µ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
        return df
    
    # –ù–∞–π—Ç–∏ –∫–æ–ª–æ–Ω–∫—É –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
    qty_col = find_column(df, ['qty', '_merged_qty_', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–ö–æ–ª-–≤–æ', '–∫–æ–ª-–≤–æ'])
    if not qty_col or qty_col not in df.columns:
        print("‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–∫–ª—é—á–µ–Ω–∏—è –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
        return df
    
    excluded_count = 0
    reduced_count = 0
    
    for exclude_name, exclude_qty in exclude_items:
        # –ù–∞–π—Ç–∏ —Å—Ç—Ä–æ–∫–∏ —Å —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º (—á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
        mask = df[desc_col].astype(str).str.contains(exclude_name, case=False, na=False, regex=False)
        matching_indices = df[mask].index.tolist()
        
        if not matching_indices:
            print(f"‚ö†Ô∏è –≠–ª–µ–º–µ–Ω—Ç '{exclude_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ BOM")
            continue
        
        remaining_exclude_qty = exclude_qty
        
        for idx in matching_indices:
            if remaining_exclude_qty <= 0:
                break
            
            current_qty = df.loc[idx, qty_col]
            if pd.isna(current_qty):
                continue
            
            try:
                current_qty = int(current_qty)
            except (ValueError, TypeError):
                continue
            
            if current_qty <= remaining_exclude_qty:
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
                item_name = df.loc[idx, desc_col]
                # –£–¥–∞–ª–∏—Ç—å –≤—Å—é —Å—Ç—Ä–æ–∫—É
                df = df.drop(idx)
                remaining_exclude_qty -= current_qty
                excluded_count += 1
                print(f"‚úì –ò—Å–∫–ª—é—á–µ–Ω —ç–ª–µ–º–µ–Ω—Ç '{item_name}' (qty: {current_qty})")
            else:
                # –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                new_qty = current_qty - remaining_exclude_qty
                df.loc[idx, qty_col] = new_qty
                print(f"‚úì –£–º–µ–Ω—å—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ '{df.loc[idx, desc_col]}': {current_qty} ‚Üí {new_qty}")
                remaining_exclude_qty = 0
                reduced_count += 1
        
        if remaining_exclude_qty > 0:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–∫–ª—é—á–∏—Ç—å –ø–æ–ª–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ '{exclude_name}': –æ—Å—Ç–∞–ª–æ—Å—å {remaining_exclude_qty}")
    
    if excluded_count > 0 or reduced_count > 0:
        print(f"\nüìä –ò—Ç–æ–≥–æ –∏—Å–∫–ª—é—á–µ–Ω–æ: {excluded_count} —Å—Ç—Ä–æ–∫, —É–º–µ–Ω—å—à–µ–Ω–æ: {reduced_count} —Å—Ç—Ä–æ–∫")
    
    return df


def process_file_for_comparison(file_path: str, no_interactive: bool = True) -> Dict[str, pd.DataFrame]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç BOM —Ñ–∞–π–ª –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–Ω–æ—Å–æ–º unclassified –≤ 'others')
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        no_interactive: –û—Ç–∫–ª—é—á–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å DataFrame
    """
    print(f"\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {file_path}")
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª
    df = load_and_combine_inputs([file_path], None, None)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏
    df, ref_col, desc_col, value_col, part_col, qty_col, mr_col = normalize_and_merge_columns(df)
    
    # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    if desc_col in df.columns:
        df = df[df[desc_col].notna() & (df[desc_col].astype(str).str.strip() != '')]
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
    has_existing_category = 'category' in df.columns
    
    if not has_existing_category:
        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å
        df = run_classification(df, ref_col, desc_col, value_col, part_col, loose=False)
        
        # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ JSON
        df = apply_rules_from_json(df, "rules.json", desc_col, value_col, part_col, ref_col)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ unclassified –≤ 'others'
        unclassified_mask = df["category"] == "unclassified"
        unclassified_count = unclassified_mask.sum()
        if unclassified_count > 0:
            print(f"‚ÑπÔ∏è  –ü–µ—Ä–µ–Ω–æ—Å {unclassified_count} –Ω–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—é '–î—Ä—É–≥–∏–µ'")
            df.loc[unclassified_mask, "category"] = "others"
    
    # –û—á–∏—Å—Ç–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è
    if not has_existing_category:
        from .formatters import clean_component_name
        if desc_col in df.columns:
            cleaned_values = []
            for val in df[desc_col]:
                if pd.notna(val):
                    cleaned_values.append(clean_component_name(str(val)))
                else:
                    cleaned_values.append(val)
            df[desc_col] = cleaned_values
    
    # –°–æ–∑–¥–∞—Ç—å outputs —Å–ª–æ–≤–∞—Ä—å
    outputs = create_outputs_dict(df)
    
    # –í–ê–ñ–ù–û: –ü—Ä–∏–º–µ–Ω–∏—Ç—å format_excel_output –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    # –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –¥–∞–Ω–Ω—ã–µ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É (–∏–∑–≤–ª–µ–∫–∞–µ—Ç –¢–£, –¥–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫–∏, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç)
    from .excel_writer import format_excel_output, RUS_SHEET_NAMES
    processed_outputs = {}
    
    for category, cat_df in outputs.items():
        if not cat_df.empty:
            # –ü–æ–ª—É—á–∏—Ç—å —Ä—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            sheet_name = RUS_SHEET_NAMES.get(category, category)
            
            # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø–æ–ª–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¢–£, –æ—á–∏—Å—Ç–∫–∞, —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞)
            # force_reprocess=True: –≤—Å–µ–≥–¥–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∑–∞–Ω–æ–≤–æ, –¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
            processed_df = format_excel_output(
                cat_df, 
                sheet_name, 
                desc_col,
                force_reprocess=True
            )
            processed_outputs[category] = processed_df
        else:
            processed_outputs[category] = cat_df
    
    print(f"‚úì –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(df)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ {len(outputs)} –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö")
    
    return processed_outputs


def compare_bom_files(file1_path: str, file2_path: str, output_path: str, no_interactive: bool = True):
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ BOM —Ñ–∞–π–ª–∞ –∏ —Å–æ–∑–¥–∞–µ—Ç –æ—Ç—á–µ—Ç –æ —Ä–∞–∑–ª–∏—á–∏—è—Ö
    
    Args:
        file1_path: –ü—É—Ç—å –∫ –ø–µ—Ä–≤–æ–º—É —Ñ–∞–π–ª—É (–±–∞–∑–æ–≤—ã–π)
        file2_path: –ü—É—Ç—å –∫–æ –≤—Ç–æ—Ä–æ–º—É —Ñ–∞–π–ª—É (–Ω–æ–≤—ã–π)
        output_path: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        no_interactive: –û—Ç–∫–ª—é—á–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    """
    print("=" * 80)
    print("üîÑ –°–†–ê–í–ù–ï–ù–ò–ï BOM –§–ê–ô–õ–û–í")
    print("=" * 80)
    
    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–±–∞ —Ñ–∞–π–ª–∞
    outputs1 = process_file_for_comparison(file1_path, no_interactive)
    outputs2 = process_file_for_comparison(file2_path, no_interactive)
    
    # –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    all_categories = sorted(set(list(outputs1.keys()) + list(outputs2.keys())))
    
    print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º...")
    
    # –°–æ–∑–¥–∞—Ç—å —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    comparison_results = []
    
    for category in all_categories:
        df1 = outputs1.get(category, pd.DataFrame())
        df2 = outputs2.get(category, pd.DataFrame())
        
        if df1.empty and df2.empty:
            continue
        
        # –ù–∞–π—Ç–∏ –∫–æ–ª–æ–Ω–∫—É –æ–ø–∏—Å–∞–Ω–∏—è
        desc_col1 = find_column(df1, ['–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ò–í–ü', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏–≤–ø', 'description', '_merged_description_']) if not df1.empty else None
        desc_col2 = find_column(df2, ['–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ò–í–ü', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏–≤–ø', 'description', '_merged_description_']) if not df2.empty else None
        
        # –ù–∞–π—Ç–∏ –∫–æ–ª–æ–Ω–∫—É –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
        qty_col1 = find_column(df1, ['–ö–æ–ª-–≤–æ', 'qty', '_merged_qty_', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ']) if not df1.empty else None
        qty_col2 = find_column(df2, ['–ö–æ–ª-–≤–æ', 'qty', '_merged_qty_', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ']) if not df2.empty else None
        
        # –°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: –Ω–∞–∑–≤–∞–Ω–∏–µ -> –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        items1 = {}
        if not df1.empty and desc_col1 and qty_col1:
            for _, row in df1.iterrows():
                name = str(row[desc_col1]) if pd.notna(row[desc_col1]) else ""
                qty_val = row[qty_col1]
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, NaN –∏ —Å—Ç—Ä–æ–∫
                if pd.notna(qty_val) and str(qty_val).strip():
                    try:
                        qty = int(float(qty_val))
                    except (ValueError, TypeError):
                        qty = 0
                else:
                    qty = 0
                items1[name] = items1.get(name, 0) + qty
        
        items2 = {}
        if not df2.empty and desc_col2 and qty_col2:
            for _, row in df2.iterrows():
                name = str(row[desc_col2]) if pd.notna(row[desc_col2]) else ""
                qty_val = row[qty_col2]
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, NaN –∏ —Å—Ç—Ä–æ–∫
                if pd.notna(qty_val) and str(qty_val).strip():
                    try:
                        qty = int(float(qty_val))
                    except (ValueError, TypeError):
                        qty = 0
                else:
                    qty = 0
                items2[name] = items2.get(name, 0) + qty
        
        # –ù–∞–π—Ç–∏ —Ä–∞–∑–ª–∏—á–∏—è
        all_items = set(list(items1.keys()) + list(items2.keys()))
        
        for item_name in sorted(all_items):
            if not item_name:
                continue
            
            qty1 = items1.get(item_name, 0)
            qty2 = items2.get(item_name, 0)
            
            if qty1 != qty2:
                if qty1 == 0:
                    # –î–æ–±–∞–≤–ª–µ–Ω
                    comparison_results.append({
                        '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': category,
                        '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': '–î–æ–±–∞–≤–ª–µ–Ω–æ',
                        '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ò–í–ü': item_name,
                        '–ö–æ–ª-–≤–æ –≤ —Ñ–∞–π–ª–µ 1': qty1,
                        '–ö–æ–ª-–≤–æ –≤ —Ñ–∞–π–ª–µ 2': qty2,
                        '–†–∞–∑–Ω–∏—Ü–∞': qty2 - qty1
                    })
                elif qty2 == 0:
                    # –£–¥–∞–ª–µ–Ω
                    comparison_results.append({
                        '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': category,
                        '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': '–£–¥–∞–ª–µ–Ω–æ',
                        '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ò–í–ü': item_name,
                        '–ö–æ–ª-–≤–æ –≤ —Ñ–∞–π–ª–µ 1': qty1,
                        '–ö–æ–ª-–≤–æ –≤ —Ñ–∞–π–ª–µ 2': qty2,
                        '–†–∞–∑–Ω–∏—Ü–∞': qty2 - qty1
                    })
                else:
                    # –ò–∑–º–µ–Ω–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    comparison_results.append({
                        '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': category,
                        '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': '–ò–∑–º–µ–Ω–µ–Ω–æ',
                        '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ò–í–ü': item_name,
                        '–ö–æ–ª-–≤–æ –≤ —Ñ–∞–π–ª–µ 1': qty1,
                        '–ö–æ–ª-–≤–æ –≤ —Ñ–∞–π–ª–µ 2': qty2,
                        '–†–∞–∑–Ω–∏—Ü–∞': qty2 - qty1
                    })
    
    # –°–æ–∑–¥–∞—Ç—å DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    if comparison_results:
        result_df = pd.DataFrame(comparison_results)
        
        # –ó–∞–ø–∏—Å–∞—Ç—å –≤ Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            result_df.to_excel(writer, sheet_name='–°—Ä–∞–≤–Ω–µ–Ω–∏–µ', index=False)
            
            # –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Å—Ç–∏–ª–∏
            from .excel_writer import apply_excel_styles
            apply_excel_styles(writer)
        
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∑–∞–ø–∏—Å–∞–Ω—ã: {output_path}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Ä–∞–∑–ª–∏—á–∏–π: {len(comparison_results)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        added = len([r for r in comparison_results if r['–ò–∑–º–µ–Ω–µ–Ω–∏–µ'] == '–î–æ–±–∞–≤–ª–µ–Ω–æ'])
        removed = len([r for r in comparison_results if r['–ò–∑–º–µ–Ω–µ–Ω–∏–µ'] == '–£–¥–∞–ª–µ–Ω–æ'])
        changed = len([r for r in comparison_results if r['–ò–∑–º–µ–Ω–µ–Ω–∏–µ'] == '–ò–∑–º–µ–Ω–µ–Ω–æ'])
        
        print(f"   –î–æ–±–∞–≤–ª–µ–Ω–æ: {added}")
        print(f"   –£–¥–∞–ª–µ–Ω–æ: {removed}")
        print(f"   –ò–∑–º–µ–Ω–µ–Ω–æ: {changed}")
    else:
        print("\n‚úÖ –§–∞–π–ª—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã, —Ä–∞–∑–ª–∏—á–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        # –í—Å–µ —Ä–∞–≤–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª —Å —Å–æ–æ–±—â–µ–Ω–∏–µ–º
        result_df = pd.DataFrame([{'–†–µ–∑—É–ª—å—Ç–∞—Ç': '–§–∞–π–ª—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã, —Ä–∞–∑–ª–∏—á–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}])
        result_df.to_excel(output_path, sheet_name='–°—Ä–∞–≤–Ω–µ–Ω–∏–µ', index=False)


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI
    """
    parser = argparse.ArgumentParser(description="BOM Categorizer CLI")
    parser.add_argument("--inputs", nargs="+", help="–í—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã (TXT, DOCX, XLSX)")
    parser.add_argument("--sheets", help="–õ–∏—Å—Ç—ã Excel (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    parser.add_argument("--sheet", help="–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ª–∏—Å—Ç Excel")
    parser.add_argument("--xlsx", help="–í—ã—Ö–æ–¥–Ω–æ–π Excel —Ñ–∞–π–ª")
    parser.add_argument("--compare", nargs=2, metavar=('FILE1', 'FILE2'), help="–°—Ä–∞–≤–Ω–∏—Ç—å –¥–≤–∞ BOM —Ñ–∞–π–ª–∞")
    parser.add_argument("--compare-output", help="–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    parser.add_argument("--txt-dir", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è TXT –æ—Ç—á–µ—Ç–æ–≤")
    parser.add_argument("--combine", action="store_true", help="–°–æ–∑–¥–∞—Ç—å SUMMARY –ª–∏—Å—Ç")
    parser.add_argument("--loose", action="store_true", help="–ù–µ—Å—Ç—Ä–æ–≥–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
    parser.add_argument("--interactive", action="store_true", help="–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
    parser.add_argument("--no-interactive", action="store_true", help="–û—Ç–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º")
    parser.add_argument("--assign-json", default="rules.json", help="JSON —Ñ–∞–π–ª —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏")
    parser.add_argument("--exclude-items", help="–§–∞–π–ª —Å —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è (—Ñ–æ—Ä–º–∞—Ç: –ù–∞–∑–≤–∞–Ω–∏–µ –ò–í–ü, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)")
    
    args = parser.parse_args()
    
    # –†–µ–∂–∏–º —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    if args.compare:
        if not args.compare_output:
            print("‚ùå –û—à–∏–±–∫–∞: —É–∫–∞–∂–∏—Ç–µ --compare-output –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            return
        compare_bom_files(args.compare[0], args.compare[1], args.compare_output, args.no_interactive)
        return
    
    # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
    if not args.inputs or not args.xlsx:
        print("‚ùå –û—à–∏–±–∫–∞: —É–∫–∞–∂–∏—Ç–µ --inputs –∏ --xlsx –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤")
        return
    
    # Load and combine inputs
    print(f"–ó–∞–ø—É—Å–∫: split_bom --inputs {' '.join(args.inputs)} --xlsx {args.xlsx} {' --combine' if args.combine else ''} {' --txt-dir ' + args.txt_dir if args.txt_dir else ''}")
    
    df = load_and_combine_inputs(args.inputs, args.sheets, args.sheet)
    
    # Normalize and merge columns
    df, ref_col, desc_col, value_col, part_col, qty_col, mr_col = normalize_and_merge_columns(df)
    
    # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ)
    if args.exclude_items:
        print(f"\nüîß –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞: {args.exclude_items}")
        exclude_items = parse_exclude_items(args.exclude_items)
        if exclude_items:
            print(f"–ù–∞–π–¥–µ–Ω–æ {len(exclude_items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è")
            df = apply_exclusions(df, exclude_items, desc_col)
            df = df.reset_index(drop=True)
    
    # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –î–û –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ø–∞–¥–∞–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –≤ "unclassified"
    if desc_col in df.columns:
        initial_count = len(df)
        df = df[df[desc_col].notna() & (df[desc_col].astype(str).str.strip() != '')]
        filtered_count = initial_count - len(df)
        if filtered_count > 0:
            print(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_count} —Å—Ç—Ä–æ–∫ —Å –ø—É—Å—Ç—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∫–æ–ª–æ–Ω–∫–∞ category (—Ñ–∞–π–ª –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Ä–∞–Ω–µ–µ)
    has_existing_category = 'category' in df.columns
    
    if has_existing_category:
        print("‚úì –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–æ–Ω–∫–∞ 'category' (—Ñ–∞–π–ª —É–∂–µ –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Ä–∞–Ω–µ–µ).")
        print("  –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.")
        # –ù–ï —É–¥–∞–ª—è–µ–º –∏ –ù–ï –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é!
        # –î–∞–Ω–Ω—ã–µ —É–∂–µ –æ—á–∏—â–µ–Ω—ã –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã, –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ —É—Ö—É–¥—à–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    else:
        # Run classification —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        df = run_classification(df, ref_col, desc_col, value_col, part_col, args.loose)
    
    # Apply existing rules from JSON (—Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤)
    if not has_existing_category:
        df = apply_rules_from_json(df, args.assign_json, desc_col, value_col, part_col, ref_col)
    
    # Interactive classification if needed
    unclassified_count = len(df[df["category"] == "unclassified"])
    auto_interactive = unclassified_count > 0 and not args.interactive and not args.no_interactive
    
    if args.interactive or auto_interactive:
        df = interactive_classification(df, desc_col, value_col, part_col, args.assign_json, auto_prompted=auto_interactive)
    
    # –û—á–∏—Å—Ç–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¢–û–õ–¨–ö–û –¥–ª—è –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
    # –î–ª—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –æ—á–∏—â–µ–Ω—ã
    if not has_existing_category:
        from .formatters import clean_component_name
        if desc_col in df.columns:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º clean_component_name –∫–æ –≤—Å–µ–º –∑–Ω–∞—á–µ–Ω–∏—è–º
            cleaned_values = []
            for val in df[desc_col]:
                if pd.notna(val):
                    cleaned_values.append(clean_component_name(str(val)))
                else:
                    cleaned_values.append(val)
            df[desc_col] = cleaned_values
    
    # Create outputs dictionary
    outputs = create_outputs_dict(df)
    
    # Re-apply rules after interactive classification (outputs need to be updated)
    if args.interactive or auto_interactive:
        # Re-create outputs to reflect new classifications
        outputs = create_outputs_dict(df)
    
    # Print summary
    print_summary(outputs)
    
    # Write Excel
    write_categorized_excel(outputs, df, args.xlsx, args.combine, desc_col)
    
    # Write TXT reports
    if args.txt_dir:
        write_txt_reports(outputs, args.txt_dir, desc_col)
    
    print("–ì–æ—Ç–æ–≤–æ.")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
        sys.exit(1)
    except Exception as e:
        print(f"\n–û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
